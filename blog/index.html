<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Selective Regional Adaptive Sampling for Diffusion Models: Exploring Region Scoring Methods</title>
    <style>

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f5f5f5;
        }

        .article {
            background-color: white;
            padding: 2rem;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
        }

        .header {
            text-align: center;
            margin-bottom: 2rem;
        }

        h1 {
            font-size: 2.5rem;
            color: #2c3e50;
            margin-bottom: 0.5rem;
        }

        .metadata {
            color: #666;
            font-size: 0.9rem;
        }

        section {
            margin-bottom: 2rem;
        }

        h2 {
            color: #2c3e50;
            border-bottom: 2px solid #eee;
            padding-bottom: 0.5rem;
            margin-top: 2rem;
        }

        h3 {
            color: #34495e;
            margin-top: 1.5rem;
            font-size: 1.3rem;
        }


        a {
            color: #3498db;
            text-decoration: none;
        }

        a:hover {
            text-decoration: underline;
        }

        .references {
            border-top: 2px solid #eee;
            margin-top: 3rem;
            padding-top: 1rem;
        }

        .references ol {
            padding-left: 1.5rem;
        }

        .references li {
            margin-bottom: 0.5rem;
            position: relative;
            padding-left: 0.5rem;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f5f5f5;
        }

        .article {
            background-color: white;
            padding: 2rem;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
        }


        .toc {
            background-color: #f8f9fa;
            padding: 1rem;
            border-radius: 4px;
            margin-bottom: 2rem;
        }

        .toc ul {
            list-style-type: none;
            padding-left: 0;
        }

        .toc > ul > li {
            margin-bottom: 1rem;
            font-size: 1.1rem;
            font-weight: 600;
        }

        .toc > ul > li > a {
            color: #2c3e50;
        }

        .toc li ul {
            padding-left: 1.5rem;
            margin-top: 0.5rem;
        }

        .toc li ul li {
            margin-bottom: 0.5rem;
            font-size: 0.95rem;
            font-weight: normal;
        }

        .header {
            text-align: center;
            margin-bottom: 2rem;
        }

        h1 {
            font-size: 2.5rem;
            color: #2c3e50;
            margin-bottom: 0.5rem;
        }

        .metadata {
            color: #666;
            font-size: 0.9rem;
        }


        .citation {
            font-size: 0.8em;
            vertical-align: super;
            color: #3498db;
            text-decoration: none;
            margin: 0 2px;
        }

        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 1rem auto;
        }

        @media (max-width: 600px) {
            body {
                padding: 10px;
            }
            
            .article {
                padding: 1rem;
            }

            h1 {
                font-size: 2rem;
            }
        }
    </style>
</head>
<body>
    <article class="article">
        <header class="header">
            <h1>Selective Regional Adaptive Sampling: Exploring Region Scoring Methods Efficient Diffusion Sampling</h1>
            <div class="metadata">
                <span>Published on: December 9, 2025</span>
                <span> • </span>
                <span>By: Jessica Lu, Jenny Baek, and Cynthia Cao</span>
            </div>
        </header>

        <section id="introduction">
            <h2>Introduction</h2>

            <img src="intro_imgs.png" alt="intro_imgs.png">
            
            <p>Although diffusion models have become prominent in text-to-image generation, they are slow and computationally expensive to run inference with. The denoising process relies on iterative forward passes, where each pass requires attention operations that involve heavy computation across all text and image tokens. This project investigates how different region selection methods influence RAS, a novel diffusion sampling strategy that improves sampling efficiency by updating only a subset of spatial tokens at each step [1]. In the original RAS method, these regions are selected using predicted-noise standard deviation as a scoring heuristic. We explore alternative region-scoring metrics, such as attention-based and edge-based methods, which we believe may better capture semantic importance or fine-grained structure by incorporating more information from attention weights or neighboring tokens.</p>

            <p>We demonstrate that standard deviation is a brittle scoring metric for the RAS method, particularly at low sampling ratios and on realistic, multi-object prompts, often failing to prioritize semantically meaningful regions.  We evaluate how these metrics influence token-selection behavior and generation quality. While quantitative results across metrics are similar, our human study indicates that attention-based scoring produces more promising results for semantically coherent and human-preferred images in low-sampling regimes and realistic image prompts.</p>
        </section>

        <section id="relevant-work">
            <h2>Relevant Work</h2>
            <h3 id="sd3">Stable Diffusion 3</h3>
                <p>The noisy latent image is first patchified into image tokens, and the text prompt is made into text tokens. Then, the Multimodal Diffusion Transformer (MMDiT) processes them simultaneously through transformer blocks, with a separate set of weights used for processing each modality. Within each MMDiT block, the joint attention mechanism allows information to flow between image and text tokens within the same attention layers. This enables SD3 to generate images that closely represent the prompt, and hone visual details like backgrounds and lettering [2].</p>
            
            <h3 id="sd3">RAS</h3>
            <p>Region-Adaptive Sampling (RAS) is a novel, training-free method for accelerating sampling for Diffusion Transformers (DiTs). Many previous sampling acceleration methods rely on reducing the number of overall sampling steps [2]. RAS employs a different approach, reducing computation within each step by updating only a subset of tokens. Because DiTs treat the latent as a sequence of independent tokens with learned positional embeddings, RAS is able to freely prune, reorder, or cache tokens during inference without violating architectural assumptions. 
            </p>
            <p>The authors observed that during the sampling process, DiTs exhibit: </p>
            <ul>
                <li>temporal continuity: regions receiving the strongest updates tend to remain important across adjacent steps.</li>
                <li>lower variance within channels in predicted noise tokens in foreground regions.</li>
            </ul>
            <p>Building off these insights, RAS computes a score for each image patch at each sampling step based on the standard deviation of the model’s predicted noise. The lowest-std patches are selected as fast-update tokens and passed through the next DiT forward pass, while slow-update tokens are skipped for that step and simply reuse cached noise from the previous iteration. (See Fig. 1) After merging the updated and cached predictions, sampling proceeds normally. To prevent starving any region, RAS tracks drop counts to periodically bump up the scores of neglected patches and inserts occasional dense (full-token) steps to combat accumulated errors.</p>
            <img src="./ras.png" alt="ras.png" width="500" height="auto">
            <p>The paper reports that even with a sampling ratio of 0.50 (only 50% of the image patches get updated at each step), RAS produces images with comparable quality while achieving a 1.6x speed up compared to standard full sampling. However, there are some limitations. 
            </p>
            <ul>
                <li>The std scoring metric may correlate with foreground structure, but it does not necessarily capture semantic importance. 
                </li>
                <li>It scores each token independently and does not consider token-to-token relationships. 
                </li>
                <li>Images with multiple foreground objects or heavy texture may have ambiguous noise patterns, making region selection less reliable. 
                </li>
            </ul>
            <p>This motivates our study to extend the RAS method to alternative scoring metrics.</p>
            
            <h3 id="other-diff">Other Diffusion Sampling Acceleration Methods</h3>
            <p>Outside of RAS, there is substantial work on accelerating inference in diffusion models. Many approaches focus on speeding up the sampling process. Training-based methods such as progressive distillation [3] and rectified flow [4] reduce the number of denoising steps by either distilling a teacher model’s multi-step sampler into a few steps or by learning a deterministic ODE that maps noise directly to data. Non-training methods, including DPM-Solver [3] and DeepCache [4], accelerate sampling through improved numerical integration or by caching intermediate features for reuse across steps. However, these techniques typically treat all spatial regions uniformly during sampling and do not account for spatial variation in how different regions evolve.
            </p>
            <p>Other approaches aim to reduce computation by modifying the token representation itself. Token merging [5], for example, combines semantically or structurally similar tokens to reduce the number of processed tokens. While effective at lowering computational cost, this merging can lead to information loss over the sampling trajectory.
            </p>
            <p>RAS offers an alternative to these methods, given that it dynamically selects which regions to refine at each sampling step and preserves token representations across steps rather than merging or discarding them.
            </p>

        </section>

        <section id="methods">
            <h2>Methods</h2>
            <h3 id="data">Data</h3>
            <p>We use the MS-COCO 2017 dataset [6], with a similar setup to the original paper. From this dataset, we selected 100 prompt–image pairs. Prompts were chosen according to two criteria: first, the baseline Stable Diffusion 3 model needed to produce reasonably high-quality images for the prompt to ensure meaningful comparisons across sampling methods, and second, the set of prompts should span a wide variety of scene types.
            </p>

            <h3 id="archichitectures">RAS Implementation</h3>
            <p>We build on the official RAS implementation for Stable Diffusion 3 by modifying the token scoring function used to select fast-update tokens. We evaluate two alternative scoring methods: attention-based and edge-based.</p>

            <h4>How RAS selects tokens</h4>
                <ul>
                    <li>At each selective sampling step, tokens are ranked by a scalar <b>score</b>.
                    </li>
                    <li>A user-defined parameter <b>sampling_ratio</b> determines what fraction of tokens receive updates.
                    </li>
                    <li>The original paper computes scores using the per-token <b>standard deviation</b> across channels of the predicted noise from the previous step.
                    </li>
                </ul>
            <h4>Starvation Prevention</h4>
            <p>RAS also performs starvation prevention, where they combat against some tokens rarely or never updating. They do 3 things to accomplish this.
            </p>
            <ul>
                <li><b>drop_count</b> weighted score function. 
                    <ul>
                    <li>RAS tracks how many times each token has been dropped and rescales score.</li>
                    <li>
                        <img src="./ras_score_eq.png" alt="ras_score_eq.png" width="200" height="auto" style="display:block; margin-left:0;">
                    </li>
                    </ul>
                </li>
                <li>
                    <b>high_ratio</b> exploration.
                    <ul>
                    <li>Each step selects a mixture of top-scoring and low-scoring tokens. (high_ratio = 0.3 means 70% highest scoring + 30% lowest scoring tokens are selected.)
                    </li>
                    </ul>
                </li>
                <li><b>error_reset_steps</b>
                    <ul>
                    <li>Specified sampling steps where all tokens are updated.
                    </li>
                    </ul>
                </li>
                </ul>
            <h4>RAS settings used in experiments: </h4>
            <p>(best-performing settings reported in the paper)</p>
            <ul>
                    <li>num_inference_steps = 28
                    </li>
                    <li>RAS sampling begins at step 4
                    </li>
                    <li>error_reset_steps = {12, 20} 
                    </li>
                    <li>high_ratio = 0.30 </li>
            </ul>

            <h3 id="prelim">Preliminary Experiments</h3>
            <h4> Evaluating RAS-std at Low Sampling Ratios</h4>
            <p>As a baseline, we compare the original RAS scoring metric (predicted-noise std) against a random-token selection baseline at low sampling ratios (50%, 25%). Across several MS-COCO prompts, we visually inspect the outputs and observe that RAS-std often performs similarly to RAS-random. In multi-object or more realistic scenes, RAS-std is often worse, producing blurred or unstable foreground objects. These results suggest that std is a weak proxy for semantic importance, motivating alternative scoring functions.</p>
            <h4>Inspecting Attention Maps in SD3</h4>
            <p>We visualize attention maps from several SD3 transformer blocks to understand whether attention patterns correspond to semantically important regions. We observe that some blocks (especially early-mid layers, 6-10) highlight consistent semantic regions such as faces, animals, and main objects, while others emphasize primarily structural edges. This suggests that attention may provide a stronger, more semantically grounded scoring signal than std. These insights inform our choice of attention blocks for our custom scoring method.
            </p>
            <p>See Appendix for visual examples.</p>

            <h3 id="custom-scoring">Our Custom Scoring Methods</h3>
            <h4>Attention-Based Scoring</h4>
            <p>Transformer attention reveals which tokens are most influential when predicting noise. We hypothesize that tokens that many other tokens attend to correspond to semantically important regions and are a good proxy for what tokens in the image should be updated more frequently. 
            </p>
            <p>SD3 uses joint attention, where each block performs self-attention over both image and text tokens, meaning noise latent tokens and prompt embeddings are concatenated before computing. 
            </p>
            <img src="./attn_eq.png" alt="attn_eq.png" width="200" height="auto">
            <p>where Q and K are the query and key projections of all tokens, and A has one row per query and one column per key.</p>
            <p>We define an attention-based score for each image token as the column-wise sum of the attention matrix over all query tokens. 
            For each image token j:
            </p>
            <img src="./attn_score.png" alt="attn_score.png" width="180" height="auto">
            
            <p>We aggregate this score across heads and selected attention blocks.
            </p>
            <p>We explore 4 RAS-attn variants, including block groups: {6,7,8} vs. {8,9,10} and high_ratio: 0.30 vs. 0.10. Different transformer blocks are known to specialize in different aspects of the representation (e.g., global layout vs. fine detail), which is supported by our preliminary study. We also hypothesize that attention provides a sharper signal, and thus a smaller exploration term might be needed for optimal performance. 
            </p>

            <p><em>Note: our attention-based metric introduces a modest computational overhead. SD3 often uses FlashAttention, which bypasses explicitly calculating the full attention matrix at once for efficient GPU computation. However, to obtain attention scores, we must calculate the full attention matrix A for selected blocks at RAS steps. 
            </em></p>
            <p><em>In RAS, only fast-update tokens produce new Q, K, V, while slow-update keys/values come from the cache. Thus, for RAS, the key tokens include both image tokens and text tokens, and the query tokens include only the fast-update image tokens plus text tokens. Thus, for a given step, A has shape ((sampling_ratio*N)+P)×(N+P), where N is the number of image tokens and  P is the number of text tokens. 
            </em></p>

            <h4>Edge-Based Scoring
            </h3>
            <p>We additionally explored an edge-based scoring metric using Sobel and Scharr filters. These kernels estimate local spatial gradients, so they highlight regions where image intensity changes sharply. By incorporating gradient information, RAS can use not only per-token metrics but also structural cues from nearby tokens. Given that edges typically correspond to semantically meaningful structures, an edge-based scoring method might more effectively allow RAS to choose the best regions to focus on.
            </p>
            <p>In our edge-based scoring method, we apply standard edge-detection filters to each channel of the feature map and compute the L2 norm across channels to obtain a single scalar score per token. After patchifying the image, we rank patches according to this score in descending order. We experimented with both Sobel and Scharr filters, and found that Scharr produced better results, likely due to its improved rotational symmetry and more accurate gradient estimation.
            </p>

            <h3 id="#quant">Quantitative Evaluation of Results</h3>
            <p>We used CLIP (Contrastive Language-Image Pre-training), LPIPS (Learned Perceptual Image Patch Similarity), and LAION-AI’s Aesthetic Predictor V1 to evaluate the quality of our generated images. All of our quantitative measures were run with NVIDIA A100 80GB GPU.
            </p>
            <p><b>CLIP Score:</b> “On a scale of 0-1, how well did our output match the prompt?”
            </p>
            <ul>
                    <li>Measures how well a generated image matches its prompt, where a higher score means better alignment [9]. We used a ViT-B/32 Transformer architecture, which was jointly trained with an image encoder and text encoder to measure the cosine similarity between image and text embeddings, optimizing for image-text pairs that contextually fit each other best.
                    </li>
            </ul>

 
            <p><b>LPIPS:</b> “On a scale of 0-1, how much is our output visually different from Stable Diffusion’s output?”
            </p>
            <ul>
                    <li>Uses deep networks to measure the perceptual similarity between two images, where a lower score means higher similarity. Cosine distance is used to compute the score [10]. We compared our RAS variants against Stable Diffusion 3.
                    </li>
                    <li>“Perceptual similarity” means “how similar two things are based on human judgment,” rather than making a purely statistical comparison like PSNR (Peak Signal-to-Noise Ratio) and SSIM (Structural Similarity Index Measure) [10].
                    </li>
            </ul>
            <p><b>LAION-AI Aesthetic Predictor:</b> “On a scale of 0-10, how aesthetically pleasing is our output?”
            </p>
            <ul>
                <li>
                    Uses a model trained on CLIP embeddings to rank an image on perceived aesthetic quality, where a higher score insinuates that it is more aesthetically pleasing. It was trained on 4000 samples that were annoted on a scale of 0 to 10 [11].
                </li>
            </ul>

            <h4>Human Study Design</h4>
            <p>To qualitatively compare the scoring methods, we conducted a human study in which participants viewed two images generated using either vanilla SD3, standard deviation, random selection, attention weights, or Scharr edge detection. We asked users to select the image that best matched the given MS-COCO prompt and was more visually appealing overall. Participants could choose <b>“Image 1”</b>, <b>“Image 2”</b>, or <b>“Both images are about the same”</b>. The metric used to generate each image was hidden and the order of questions were randomized to minimize bias.
            </p>
            <p>We evaluated each pair of scoring methods using three unique prompts, yielding 30 total comparisons. These comparisons were drawn from a pool of 16 prompts in total; some prompts appeared in more than one method pair, but no prompt was reused within a single pair. We selected image–prompt pairs from MS-COCO 2017 to ensure coverage of diverse scene types, including objects, animals, and human activities.Because a few method pairs were missing in the initial survey, the study was run in two parts. The first part received responses from 91 participants, and the second from 32 participants.
            </p>
            <p>We compute a weighted preference score for each method. For each prompt and method pair, we count the number of votes for each method and assign a weight of 1 to a direct preference and 0.5 to a neutral (“about the same”) response. The weighted total is then divided by the number of responses for that question to obtain a normalized score.
            </p>


        <section id="results">
            <h2>4. Results</h2>
            <h3>4.1. Quantitative image quality comparison</h3>
            <p>Our best attention variants performed similarly to its other variants for CLIP and LAION, while doing slightly worse for LPIPS.</p>

            <p>Our quantitative results were guided by two questions:</p>

            <ol>
                <li>Which attention RAS variant performed the best?</li>
                <li>How well did our best attention variant perform against the random, standard deviation, and edge-detection (Scharr) variants?</li>
            </ol>

            <h4>4.1.1. All RAS variants + SD3</h4>
            <p>Across our three scores (CLIP, LPIPS, LAION), our chosen attention variants were not that significantly different from their competitors. In fact, all of our variants weren’t significantly different from each other!</p>
            
            <p>For each of the three scores, we picked our best attention-based contender (explained in 4.1.2.) and compared it against “Vanilla” SD3, Random, Standard Deviation, and Scharr (with the exception of LPIPS, which didn’t include SD3). Both of our notable differences came from the average LPIPS scores. For both sampling ratios, Standard Deviation and Scharr had higher average scores than Random and Attention. In general, all four variants had higher LPIPS scores when they used a sampling ratio of 25%, just like what we saw among the attention variants.</p>

            <figure>
                <img src="paper_all_results.png" alt="paper_all_results.png">
                <figcaption>Box plots for CLIP scores and LPIPS scores across all RAS variants and Stable Diffusion (“Vanilla”). Mean scores for each variant are displayed beneath them in the blue boxes. A & B: CLIP scores; C & D: LPIPS scores; A & C: 25% sampling ratio; B & D: 50% sampling ratio.</figcaption>
            </figure>
            
            <h4>4.1.2. Best-performing attention RAS variants</h4>
            <p>For each score, our attention variants didn’t have significantly different results from each other. However, we noticed that variants that used Blocks 6-8 had higher average scores than variants that used Blocks 8-10. Also, variants that used a sampling ratio of 25% noticeably had higher LPIPS scores than variants with 50% sampling ratio, by a difference of ~0.05 on average.</p>

            <p>Our winners for each evaluation, by average score:</p>

            <ul>
                <li><b>CLIP:</b> Attn Blocks 6-8, sr = 25, hr=30</li>
                <li><b>LPIPS:</b> Attn Blocks 6-8, sr = 25, hr=10</li>
                <li><b>LAION:</b> Attn Blocks 6-8, sr = 50, hr=10</li>
            </ul>


            <figure>
                <img src="paper_attn_results.png" alt="paper_attn_results.png">
                <figcaption>Box plots for CLIP scores and LPIPS scores across attention-based RAS variants. Mean scores for each variant are displayed beneath them in the blue boxes. A & B: CLIP scores; C & D: LPIPS scores; A & C: 25% sampling ratio; B & D: 50% sampling ratio.</figcaption>
            </figure>

            <h3>4.2. Qualitative Results</h3>

            <h4>4.2.1. Human Results</h4>

            <figure>
                <img src="survey_results.png" alt="survey_results.png">
            </figure>

            <figure>
                <img src="image_results.png" alt="image_results.png">
            </figure>

            <p>Taking the results from the human survey, we computed two sets of weighted preference scores. In the first grouping, where each RAS variant is compared directly against Vanilla SD3, Vanilla SD3 attains by far the highest score, indicating that none of the sparse sampling strategies surpass full-token sampling. This matches our expectations: when the model can sample all tokens, it has strictly more information than when it is restricted to specific regions. Among the RAS methods, Attention-based sampling degrades performance the least relative to Vanilla SD3, while Scharr and Random lie in the middle, and the standard deviation method performs worst. This ordering appears in Figure 6: with the exception of the second row, the standard deviation method tends to produce the least faithful and least visually pleasing outputs, while Attention and edge-based (Scharr) sampling most closely match the Vanilla SD3 generations.</p>

            <p>In the second grouping, where scores are computed only from pairwise comparisons between different RAS strategies (excluding Vanilla SD3), Attention again emerges as the strongest method, suggesting that it better matches the textual prompt and produces more visually appealing images than the alternatives. Random performs almost on par with Scharr here, which we hypothesize is because Random sampling often yields visually appealing images, sometimes comparable to Scharr and close to Attention, but can also fail catastrophically on certain scenes. For example, in the second row of Figure 6, Random produces an image that diverges completely from the Vanilla SD3 reference, whereas Scharr remains closer. This suggests that in scenes where foreground and background are less cleanly separated, Random sampling is more likely to miss important regions, while edge-based Scharr sampling can still focus on salient structure. Conversely, Scharr performs worst in the third row, which we attribute to the fact that convolving with a fixed edge kernel can oversmooth or distort fine details, such as the zebra’s head.</p>

            <p>Finally, we note that our overall goal is to generate high-quality images faster than vanilla SD3. The original RAS paper reports that the Standard Deviation strategy achieves a throughput 1.6× higher than Vanilla SD3. Our Attention method uses more memory, so in our setup it reaches a slightly lower speedup of 1.5×, while edge-based methods, which add an extra convolution, run at approximately the same speed as Vanilla SD3. Even so, we believe these variants are worth exploring further, given the preliminary evidence that they can improve image quality relative to the baseline.</p>



        </section>

        <section id="discussion">
            <h2 id="discussion">5. Discussion</h2>
            <h3>5.1 Key Takeaways</h3>
            <p>Our evaluations raise the question: is our attention-based scoring metric a promising alternative to the standard deviation metric used in RAS [1]?</p>
            <p>Although our RAS variants (plus SD3) didn’t have CLIP/LPIPS/LAION scores that were notably different from each other, our results point to two observations. First, averaging across attention blocks 6-8 lead to better results than Blocks 8-10, and we hypothesize that blocks within the middle of the denoising process may contain more useful information that focuses on both holistic patterns and fine details in an image, whereas later blocks will contain more information about fine details. Second, LPIPS scores were generally higher for metrics that used a 25% sampling ratio of text-to-image tokens. This may be because the RAS model was able to allocate more of its computation towards image-related information, which may have allowed it to produce outputs that were more perceptually similar to SD3’s.</p> 
            
            <h4>5.2 Limitations and Future Work</h4>
            <p>Even though CLIP score is widely used for measuring image-prompt alignment, and LPIPS is used for image-image similarity, it is worth exploring whether CLIP and LPIPS are sufficient for evaluating the visual quality of a generative image model. Because LPIPS requires a set of reference images to compare results against, the scores are biased and not standardized. Common metrics like FID (Frechet Inception Distance) experience a similar limitation. For a more thorough comparison of RAS variants, we would like to explore unbiased and sample-efficient evaluations, such as CMMD [12].</p>
            <p>Although the human study provides interesting preliminary results, it has several limitations. First, the study only included 15 prompts, which restricts our ability to generalize preferences across the full diversity of MS-COCO. Specifically, practical limits on survey length prevented us from evaluating a larger prompt set. Second, the survey was conducted in two parts due to missing comparisons in the initial version, resulting in uneven participant counts. To mitigate this, we aggregated preferences on a per-comparison basis and normalized scores, but the discrepancy in sample sizes may still introduce additional variance. Third, although a randomized prompt selection would reduce potential selection bias, it would require a substantially larger prompt set than was practical for our survey design, as mentioned earlier. Given more time, we would aim to include more prompts and increase the number of comparisons between methods within each prompt.</p>
        </section>


        <section id="references">
            <h2 id="references">References</h2>
            <ol>
                <li id="ref1">Z. Liu, Y. Yang, C. Zhang, Y. Zhang, L. Qiu, Y. You and Y. Yang, “Region-Adaptive Sampling for Diffusion Transformers,” arXiv preprint arXiv:2502.10389, 2025.</li>
                <li id="ref2">P. Esser et al., “Scaling rectified flow transformers for high-resolution image synthesis,” in Proc. 41st Int. Conf. Mach. Learn. (ICML’24), Vienna, Austria, 2024, pp. 503–530.</li>
                <li id="ref3">Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. In ACM SIGGRAPH 2022 conference proceedings, pages 1–10, 2022.</li>
                <li id="ref4">Michael S Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. arXiv preprint arXiv:2209.15571, 2022. 2023, pp. 4599–4608.</li>
                <li id="ref5">Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:5775–5787, 2022. 1, 4</li>
                <li id="ref6">Mengwei Xu, Mengze Zhu, Yunxin Liu, Felix Xiaozhu Lin, and Xuanzhe Liu. Deepcache: Principled cache for mobile deep vision. In Proceedings of the 24th annual international conference on mobile computing and networking, pages 129–144, 2018.</li>
                <li id="ref7">D. Bolya and J. Hoffman, “Token Merging for Fast Stable Diffusion,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</li>
                <li id="ref8">Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence ´ Zitnick. Microsoft coco: Common objects in context. In Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740–755. Springer, 2014.</li>
                <li id="ref9">A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger and I. Sutskever, “Learning Transferable Visual Models From Natural Language Supervision,” arXiv preprint arXiv:2103.00020, 2021.</li>
                <li id="ref10">R. Zhang, P. Isola, A. A. Efros, E. Shechtman and O. Wang, “The Unreasonable Effectiveness of Deep Features as a Perceptual Metric,” in Proc. 2018 IEEE/CVF Conf. Computer Vision and Pattern Recognition (CVPR), 2018, pp. 586–595.</li>
                <li id="ref11">“LAION-AI/aesthetic-predictor,” GitHub repository, [Online]. Available: https://github.com/LAION-AI/aesthetic-predictor.</li>
                <li id="ref12">S. Jayasumana, S. Ramalingam, A. Veit, D. Glasner, A. Chakrabarti, and S. Kumar, "Rethinking FID: Towards a better evaluation metric for image generation," arXiv preprint arXiv:2401.09603, 2024.</li>
            </ol>
        </section>

        <section id="references">
             <h2 id="references">Appendix</h2>
            <h3>Preliminary Experiment Figures</h3>

            <img src="./elephant.png" alt="elephant.png" width="500" height="auto">
            <p>These are the attention maps of SD3, visualized across a few blocks of SD3's DiT and across sampling steps, for a generated image above. </p>
            <p>The image above was generated by the prompt: <em>"A road with motorcycle riders, a bus, a person riding an elephant and people walking on the side of the road."</em></p>
            <p>Block 5</p>
            <img src="./attn_b5.png" alt="attn_b5.png" width="500" height="auto">
            <img src="./attn_b52.png" alt="attn_b52.png" width="500" height="auto">
            <p>Block 10</p>
            <img src="./attn_b10.png" alt="attn_b10.png" width="500" height="auto">
            <img src="./attn_b102.png" alt="attn_b102.png" width="500" height="auto">
            <p>Block 15</p>
            <img src="./attn_b15.png" alt="attn_b15.png" width="500" height="auto">
            <img src="./attn_b152.png" alt="attn_b152.png" width="500" height="auto">
            <p>Block 20</p>
            <img src="./attn_b20.png" alt="attn_b20.png" width="500" height="auto">
            <img src="./attn_b202.png" alt="attn_b202.png" width="500" height="auto">
            <p>Avged Across Blocks 8-10</p>
             <img src="./attn_b8-10.png" alt="attn_b8-10.png" width="500" height="auto">
            <img src="./attn_b8-102.png" alt="attn_b8-102.png" width="500" height="auto">
        </section>
    </article>
</body>
</html>
